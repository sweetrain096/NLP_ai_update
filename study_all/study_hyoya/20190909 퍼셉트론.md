## 퍼셉트론

<sup>2019.09.09 공부내용</sup>

Req. 0-2-1 : 퍼셉트론 개념 공부

- 인간의 뉴런 모델을 기초로, 입력값에 곱해지는 가중치값을 자동으로 학습하는 알고리즘인 퍼셉트론 개념에 대해 공부한다. S1P2243009-5



>  위키설명

1. 퍼셉트론은 인공신경망의 한 종류로서, 1957년에 코넬 항공 연구소의 프랑크 로젠블라트에 의해 고안되었다.
2. 이것은 가장 간단한 형태의 피드포워드 네트워크-선형분류기- 으로도 볼 수 있다.
3. 퍼셉트론이 동작하는 방식은 다음과 같다
   1. 각 노드의 가중치와 입력치를 곱한 것을 모두 합한 값이 활성함수에 의해 판단된다
   2. 그 값이 임계치(보통0)보다 크면 뉴런이 활성화되고 결과값으로 1을 출력한다
   3. 뉴런이 활성화 되지 않으면 결과값으로 -1을 출력한다
4. 마빈 민스키와 시모어 페퍼트는 저서 "퍼셉트론"에서 단층 퍼셉트론은 XOR연산이 불가능하지만, 다층 퍼셉트론으로는 XOR연산이 가능함을 보였다.



***

> 참고자료 [출처](https://blog.naver.com/samsjang/220948258166)

인공지능은 우리 사람의 뇌를 흉내내는 인공신경망과 다양한 머신러닝 알고리즘을 통해 구현 됩니다. 이런 딥러닝도 수십년전에 개념이 정립되었던 초기 인공신경망으로부터 발전된 것이라 볼 수 있다.



퍼셉트론과 퍼셉트론을 좀 더 발전시킨 아달라인은 이해하기가 다소 까다로울 수 있다.



우리 살마의 뇌는 신경계를 구성하는 주된 세포인 뉴런을 약 1000억개 정도 가지고 있으며, 뉴런들은 시냅스라는 구조를 통해 전기, 화학적 신호를 주고 받음으로써 다양한 정보를 받아들이고, 그 정보를 저장하는 기능을 수행한다.

![뉴런이미지](https://postfiles.pstatic.net/MjAxNzAzMDJfMjI1/MDAxNDg4NDMzMDM0Mzcy.0mhz3oBncFbMm7vGlY2E9AQKc_WvcJAr4O9bhkW9eOsg.dGenzy3Q_UKK0_NXRuC-y6K9fVdkkUSlxAsfSc-NyJ8g.PNG.samsjang/%EC%BA%A1%EC%B2%98.PNG?type=w2)



1943년 신경과학자인 Warren S. McCulloch과 논리학자인 Walter Pitts는 하나의 사람 뇌 신경세포를 하나의 이진(Binary)출력을 가지는 단순 논리 게이트로 설명했다.

위 그림에서 여러개의 입력 신호가 **가지돌기(Dendrite)**에 도착하면 신경세포 내에서 이들을 하나의 신호로 통합하고, 통합된 신호 값이 어떤 임계값을 초과하면 하나의 단일 신호가 생성되며, 이 신호가 **축삭돌기(Axon)**를 통해 다른 신경세포로 전달하는 것으로 이해했다. 이렇게 단순화 된 원리로 동작하는 뇌 세포를 McCulloch-Pitts뉴런(MCP 뉴런)이라 부릅니다.

1957년 코넬 항공 연구소에 근무하던 Frank Rosenblatt은 MCP 뉴런 모델을 기초로 **퍼셉트론(Perceptron) 학습 규칙**이라는 개념을 고안하게 되는데, Rosenblatt은 하나의 MCP뉴런이 출력신호를 발생할지 안할지 결정하기 위해 MCP뉴런으로 들어오는 각 입력값에 곱해지는 가중치 값을 자동적으로 학습하는 알고리즘을 제안했다.

이 알고리즘은 머신러닝의 지도학습이나 **분류(Classification)**의 맥락에서 볼 때, 하나의 샘플이 어떤 클래스에 속해 있는지 예측하는데 사용될 수 있습니다.



___

아래 그림은 Rosenblatt이 제안한 퍼셉트론 알고리즘 개념을 도식화 한 것입니다

![퍼셉트론 알고리즘 개념 도식화](https://postfiles.pstatic.net/MjAxNzA2MTlfMTA3/MDAxNDk3ODQzMjEwMTE4.jXgcNwRTVRbJEDO6JsHOxtftq_h9VxtCjeuCvDd9nCEg.-cCAbXvpIdBqfpS4DkUmpm339EeH3GO3FgnHr95ytiwg.PNG.samsjang/%EC%BA%A1%EC%B2%98.PNG?type=w2)

`순입력 함수`

* 여기서 `x0~xn`은 퍼셉트론 알고리즘으로 입력되는 값이며,
* `w0~wn`은 각각 `x0~xn`에 곱해지는 가중치입니다.

* 입력값은 보통 분류를 위한 데이터의 특성(feature)을 나타내는 값
* 이 특성값`x0~xn`에 가중치 `w0~wn`을 곱한 값을 모두 더하여 하나의 값으로 만든다
* 이 값을 만드는 함수를 **순입력 함수(net input함수)** 라고 한다

`활성 함수`

* 순 입력 함수의 결과값을 특정 임계값과 비교를 하고,
* 순입력 함수 결과값이 이 임계값보다 크면 1,
* 그렇지 않으면 -1로 출력하는 함수를 정의한다

퍼셉트론은 다수의 트레이닝 데이터를 이용하여 일종의 지도 학습을 수행하는 알고리즘 입니다.

트레이닝 데이터에는 데이터의 특성값에 대응되는 실제 결과값을 가지고 있어야 합니다. 입력되는 특성값 `x0~xn`에 대한 실제 결과값을 y라고 한다면 이 y를 활성함수에 의해 -1 또는 1로 변환합니다

이렇게 변환한 값과 퍼셉트론 알고리즘에 의해 예측된 값이 다르면 이 두개의 값이 같아질 때까지 특정식에 의해 가중치 `w0~wn`을 업데이트 합니다.



___

위 그림을 보다 단순하게 도식화 하면, 다음 그림과 같이 `입력층`, `중간층`, `출력층`과 같이 나타낼 수 있습니다.

![도식화](https://postfiles.pstatic.net/MjAxNzAzMTNfOTMg/MDAxNDg5Mzg5MjEwNTk4.qEuVghv_M-Jg_oPnLjpfJewQM0n8J06njvbMoP4Ejkwg.gDEsHLc_vT7TYX3i3nfHn4YZXiMF4-XoOOTs5J_VHcwg.PNG.samsjang/%EC%BA%A1%EC%B2%982.PNG?type=w2)

여기서 중간층을 노드 또는 뉴런이라고 부르며, 입력층은 다른 노드의 출력값이 입력값으로 입력되는 것이고, 출력층은 이 노드의 출력값이 다른 노드로 전달되는 층이라고 생각하면 됩니다.

이와 같이 중간층이 하나의 노드로 구성되어 중간층과 출력층의 구분이 없는 구조를 단순 또는 단층 퍼셉트론이라 부릅니다

중간층을 구성하는 노드가 여러개이고, 이러한 중간층이 다수로 구성되어있는 구조를 다층 퍼셉트론이라 부릅니다.



___

아래는 다층 퍼셉트론을 응용한 인공신경망 구조의 한 예시입니다

![인공신경망](https://postfiles.pstatic.net/MjAxNzAzMTNfMjI1/MDAxNDg5Mzg5NzE2Nzg0.52Kk02I2Xah733cc-yBgyYUsXaBdjx3U1bphBiPaj0Ag.1BKtHzWxnedvwrS9-CJNTTMXDYBj2bn-TjpKiWEmsrsg.PNG.samsjang/3.png?type=w2)

이러한 다층 인공신경망을 학습하는 알고리즘을 딥러닝이라고 말합니다.

이것이 퍼셉트론의 핵심입니다.

좀 더 자세히 알아봅시다







MCP 뉴런과 Rosenblatt의 퍼셉트론 모델은 사람 뇌의 단일 뉴런이 작동하는 방법을 흉내내기 위해 환원 접근법(redcutionist approach)을 이용합니다. 이는 초기 가중치를 임의의 값으로 정의하고 예측값의 활성 함수 리턴값과 실제 결과값의 활성 함수 리턴값이 동일하게 나올 때까지 가중치의 값을 계속 수정하는 방법입니다.



Rosenblatt의 초기 퍼셉트론 알고리즘을 요약하면 다음과 같습니다.

1. 입력되는 특성값에 곱해지는 가중치 **w** 값들을 모두 0 또는 작은 값으로 무작위 할당함
2. 임계값을 정의함(보통 0으로 정의합니다.)
3. 트레이닝 데이터 샘플 **x**를 순입력 함수를 이용해 가중치와 각각 곱한 후 그 총합을 구함
4. 활성 함수를 이용해 트레이닝 데이터 샘플에 대한 예측값을 -1 또는 1로 결과가 나오게 함
5. 트레이닝 데이터 샘플의 실제 결과값에 대한 활성 함수 리턴값과 4에서 나온 예측값을 비교함
6. 예측값과 결과값이 다르면 모든 가중치 **w**를 업데이트하고 3의 과정부터 다시 시작함
7. 예측값과 결과값이 동일하게 나오면 패스



자, 아래와 같이 **n**개의 특성값을 가진 **m**개의 트레이닝 데이터가 있다고 가정합니다.



![img](https://postfiles.pstatic.net/MjAxNzAzMDdfMjAg/MDAxNDg4ODE4MDc2MTg3.y7WU5OBL4tOoAacxhfjt5a4jRwkoGrIes3lKzoW9DdAg.v3P5c015Xeew2QUpualq10bKpeJF37EOL6ZMRUIDIWcg.PNG.samsjang/%EC%BA%A1%EC%B2%984.PNG?type=w2)



이 트레이닝 데이터로 퍼셉트론 알고리즘을 이용해 머신러닝을 수행하려고 합니다. Rosenblatt의 퍼셉트론 알고리즘에서 **x0**는 실제 트레이닝 데이터의 특성값과는 무관하며 **x0**의 초기값으로 보통 1로 둡니다. 퍼셉트론 알고리즘에서 **x0**를 바이어스(bias)라 부릅니다. 따라서 실제 트레이닝 데이터의 특성값은 **x1**부터 시작한다고 보면 됩니다.



이제 우리가 1로 정한 **x0**의 값과 **n**개의 트레이닝 데이터의 각 특성값 **x1~xn**과 이 특성값에 곱할 가중치 **w1~wn**의 값을 0과 1사이의 임의의 값으로 랜덤하게 할당한 후 위 표를 아래와 같이 다시 구성해 봅니다.



![img](https://postfiles.pstatic.net/MjAxNzAzMDdfNzkg/MDAxNDg4ODE4MDkxNDky.wCYwDhe6hX6fdPWnxNc4704YvFzc37p1hixvnC6IEtYg.7PW-2Q0qVr9imPBoLWq99TCiUVzI3NIAytJRdNAsRrYg.PNG.samsjang/%EC%BA%A1%EC%B2%985.PNG?type=w2)



트레이닝 데이터1~**m**까지 각 특성값에 대해 퍼셉트론 알고리즘을 구동합니다. 퍼셉트론 알고리즘에서 가중치를 업데이트 하는 식은 다음과 같이 정의됩니다.



![img](https://postfiles.pstatic.net/MjAxNzAzMDRfMjMw/MDAxNDg4NjAyNzA3OTQ1.x2jNmZwbFAUC1nRfSbUfGiLJuJY8G4XhnB9SmyKFtYcg.YRrgjWsEoX0rrkwDmpuNA89yn-DUKGGsXHc047HCeT0g.PNG.samsjang/%EC%BA%A1%EC%B2%983.PNG?type=w2)



여기서 **wj** 는 트레이닝 데이터의 **j**번째 특성값 **xj**와 곱하는 가중치이며, **y**는 트레이닝 데이터의 실제 결과값에 대한 활성 함수 리턴값, **y^**은 예측값에 대한 활성 함수 리턴값입니다. **(y - y^)** 앞에 곱한 그리스 문자 **η**는 학습률(learning rate)이라 하며, Rosenblatt의 퍼셉트론에서는 learning rate을 매우 작은 값으로 할당합니다.



퍼셉트론 알고리즘은 다음과 같은 방식으로 구동합니다.



1. **w0**~**wn**의 값을 0.0, 임계값 0.0, 에타값은 0.1로 둡니다.
2. 트레이닝 데이터1의 특성값들에 대한 예측값의 활성 함수 리턴값을 계산합니다.
3. 2에서 계산된 값이 이 실제 결과값의 활성 함수 리턴값과 같으면 가중치 업데이트 없이 다음 트레이닝 데이터에 대해 2번 과정으로 넘어갑니다.
4. 예측값의 활성 함수 리턴값이 실제 결과값의 활성 함수 리턴값과 다르면 위 식에 의해 **w0**~**wn**의 가중치를 업데이트 합니다. 
5. 트레이닝 데이터2~**m**에 대해서 2~4를 반복합니다.



위와 같은 절차로 트레이닝 데이터1~**m**까지 모든 예측값이 실제 결과값과 동일해질 때까지 반복합니다. 트레이닝 데이터1~**m**까지 예측값에 대한 활성 함수 리턴값이 실제 결과값의 활성 함수 리턴값과 동일하면 퍼셉트론 학습은 종료됩니다.











***

> 샘플 코드 [출처](https://medium.com/@thomascountz/19-line-line-by-line-python-perceptron-b6f113b161f3)

```python
import numpy as np

class Perceptron(object):

    def __init__(self, no_of_inputs, threshold=100, learning_rate=0.01):
        self.threshold = threshold
        self.learning_rate = learning_rate
        self.weights = np.zeros(no_of_inputs + 1)
           
    def predict(self, inputs):
        summation = np.dot(inputs, self.weights[1:]) + self.weights[0]
        if summation > 0:
          activation = 1
        else:
          activation = 0            
        return activation

    def train(self, training_inputs, labels):
        for _ in range(self.threshold):
            for inputs, label in zip(training_inputs, labels):
                prediction = self.predict(inputs)
                self.weights[1:] += self.learning_rate * (label - prediction) * inputs
                self.weights[0] += self.learning_rate * (label - prediction)
```



